\chapter{From Batch to Incremental 3D Reconstruction}

\section{Image formation}



3D Reconstruction represents a long-standing topic of interest in both the Computer Vision and Robotics communities.



The reconstruction in computer vision aims at estimating an as accurate as possible map of the environment


\section{Sparse Scene Structure Reconstruction and Camera Pose estimation}
Both Structure from Motion (SfM) in Computer Vision and Simultaneous Localization and Mapping (SLAM) in Robotics face the problem of reconstructing a sparse map of the environment together with the pose (position and orientation) of the camera capturing the scene.
The early researches of the two communities, focused on different aspects of such problem.
SfM aims in particular at  estimating accurately the map (structure) of a generic set of images, it had no particular time constraints and processed all the images in the same time (batch).
On the other hand, SLAM was thought to be deployed on a robot, therefore, the main goal of is to obtain a fast and accurate estimate of the robot pose with respect to the environment by processing a video sequence: the SLAM algorithms needs to perform in real-time, i.e., at the same rate of the camera, and incrementally, i.e., a new image  has to be included in the estimation as soon as it becomes available.


These different approaches to the same problem, led the researchers of the two communities to develop deeply different algorithm relying on different estimation tools.
Classical Structure from Motion algorithms \cite{triggs2000bundle,sibley2009adaptive,wu2011multicore} extract a set of 2D points for each image with a descriptor associated, such as SIFT \cite{sift} or ORB \cite{orb}, or others, then they find the correspondences of these 2D point among all the images that are generated by the same 3D point. Bootstrapping from an initial guess, they globally optimize the pose of the camera and the 3D points position, by minimizing the error between the 2D points (the measurements) and the the projection of the current estimate of the 3D points on the cameras.
If we deal with $n$ cameras, $k$ 3D points, we express $x_{ij}^{2D}$ the measurement of point $i$ in image $j$, and $\Pi_j(x_i^{3D})$ projects the 3D point $i$ on the image plane of the $j$-th camera, SfM aims at minimizing the following:
\[
\sum_{k}^{i=0}\sum_{n}^{j=0}||x_{ij}^{2D} - \Pi_j(x_i^{3D})||^2.
\]
It is usually minimized by Gauss-Newton or Levenberg-Marquardt algorithms, and the process is usually named as Bundle Adjustment, since the minimization process adjusts the bundles of camera-to-point, by moving the camera poses and 3D point positions. This estimation usually requires a big computational effort.


Instead, the SLAM approach \cite{davison2007monoslam,ceriani2014single,grasa2011ekf}, classically adopts a different point of view, focusing on the efficiency of the estimate, and so, it uses a different estimation tool, i.e., the Extended Kalman Filter (EKF) which estimates iteratively a state, composed by the camera pose with respect to the world together with the map of the environment, by incorporating the new measurement and marginalizing the old camera poses.  


Thanks to the Parallel Tracking and Mapping (PTAM) algorithm, proposed in \cite{klein_murray07}, these two approaches become closer and closer. PTAM proposed a SLAM system that adopts the bundle adjustment optimization: it decouples the fast tracking process, i.e., the camera pose estimation, and the slow mapping processes in two parallel thread.  
The tracking processes each frame of the video sequence while the mapping works only on keyframes. 
This new paradigm, named keyframe-SLAM, gain more and more interest in the robotics community, such that it become more popular with respect to the filtering approach \cite{strasdat2010real}.
Moreover, the improvements on the bundle adjustment optimization process proposed in \cite{kaess2008isam},  the formalization of the SLAM problem as a factor graph\cite{thrun2006graph}, and the optimization libraries g2o \cite{kummerle2011g} and GTSAM \cite{dellaert2012factor} led the researchers to move from the EKF estimation to keyframe slam based on bundle adjustment \cite{strasdat11,sunderhauf2012towards,johannsson2013temporally}. 

% LSD-SLAM

In conclusion both SfM and SLAM algorithms are able to estimate the pose of the cameras and a point cloud representing the map of the environment: the former handles a generic set of data while the latter deals with a video sequence. 
In this thesis we rely on these points and cameras estimation and we provide a system able to handle a video sequence, or a set of subsequent images, that build a very accurate, continuous and dense mesh map of the environment, exploiting the advances in Multi-View Stereo and incremental reconstruction from sparse points.

\section{Multi-View Stereo}


A wide literature in Computer Vision focuses on reconstructing a scene directly from the images by assuming that the camera poses are known, usually estimated by a Structure from Motion algorithm.
These algorithms, known as Multi-View Stereo (MVS), aims at recovering a very detailed dense and accurate reconstruction. 
While, SfM and SLAM algorithms previously described output a sparse point cloud as a representation of the map of the environment, MVS aims at reconstructing at least the big majority of the image pixels, resulting in a very dense model of the environment.

\subsection{Tools and Concepts}
Before a review of existing approaches  in Multi-View Stereo, we present some concepts that are common in almost all algorithms.
\subsubsection{Photoconsistency}
Multi-view stereo aims at recovering 3D information from a set of calibrated images. The coherence and accuracy measure to evaluate and optimize the recovered 3D data is called photo-consistency, which measures the similarity between two image regions.
Different photo-consistency measure have been used. 
Given images $I$ and $J$ and two pixels $p_0\in I$ and $q_0\in J$. Very simple measures rely on the difference between the intensity values  $I(p_0)$ and $J(q_0)$ but, taking into account only a single value, they lack in robustness. 
Robust photo-consistency measures consider a set of neighboring  pixels around $p_0$ and $q_0$, i.e., $p_i$ and $q_j$ where $1\leq i \leq n$ and $1\leq j \leq n$. Usually the neighboring pixels form a rectangular region; common measures are:
\begin{itemize}
  \item Sum of Squared Differences (SSD): $\sum_{i=0}^{n}(I(p_i) - J(q_i))^2$
  \item Sum of Absolute Differences (SAD): $\sum_{i=0}^{n}|I(p_i) - J(q_i)|$
  \item Normalized Cross-Correlation (NCC): $\frac{v_{I,J}(p_0,q_0)}{\sqrt{v_{I}(p_0} v_{J}(q_0}}})$,
  where $\mu_{I}(p_0) = \frac{\sum_{i=0}^{n}I(p_i)}{n}$ and $\mu_{J}(q_0) = \frac{\sum_{i=0}^{n}J(q_i)}{n}$ are the two mean values of the pixels considered in image $I$ and $J$;
  $v_{I}(p_0) = \frac{\sum_{i=0}^{n}I(p_i)^2}{n} - \mu_I(p_0)^2$ and $v_{J}(q_0) = \frac{\sum_{i=0}^{n}J(q_i)^2}{n} - \mu_{J}(q_0)^2$ are the variances 
  and $v_{IJ}(p_0,q_0) = \frac{\sum_{i=0}^{n}I(p_i)J(q_i)}{n} - \mu_I(p_0) \mu_{J}(q_0)$
\end{itemize}
Low values of SSD and SAD represents high photo-consistency, while NCC is the correlation between two image regions. 
The computation of the NCC measure is more complex , but the final score is robust against linear illumination changes.


\subsubsection{Regularization}
\subsubsection{Variational methods}

Since the datasets of \cite{Seitz_et_al06} and \cite{strecha2008} were made available, dense MVS has been faced with different approaches, some of which reach very accurate results on these datasets.
It is hard to provide a unique classification of MVS algorithms since the methods proposed may differ in many aspect, such as the input assumptions, the algorithm to obtain the reconstruction, optimization methods. Following the taxonomy proposed in \cite{Seitz_et_al06}, we classify the MVS algorithms according to the representation of the model: \emph{depth maps}, \emph{volumetric},  \emph{level set} and \emph{mesh} methods. The four classes does not exist alone, by themselves, but a multi-view stereo algorithm, especially the most complex and recent ones, often propose a pipeline requiring different representations of the scene. In particular the depth maps-based algorithm usually needs a meshing step achieved by volumetric or implicit surfaces methods (level set).

\subsection{Depth Maps}
A depth map is a particular image that stores for each pixel the depth of the scene.
Usually the approaches based on this representation involve two steps: reference depth maps estimation and map merging.
Given a set of images, for each pixel $i$ of a reference image $I_{\text{ref}}$, the depth estimation aims at recovering the pixel depth, by pairwise comparing the appearance of the images in a process also known as stereo matching \cite{scharstein2002taxonomy}. 
The basic idea is to look for the most likely depth $z_i$ generated from the $i$-th pixel as depicted in Figure \todo{FIGURE}.

A very popular approach compares two images as follows: for each pixel $i$ of the reference frame, it scans the corresponding epipolar line in the second image, and looks for the pixel whose neighboring patch best matches the patch around $i$  \cite{lhuillier2002match}. 
Commonly used matching costs are the SSD (Sum of Squared Differences) or the NCC (Normalized Cross Correlation).
Some algorithms first rectify the two compared images, such that the epipolar line becomes horizontal and the scanning process becomes easier 
\cite{kang2001handling,bradley2008accurate,moons20093d}. 

Another approach named plane-sweeping \cite{collins1996space}, scans the entire 3D scene, that needs to be discretized, with a fronto-parallel plane, i.e., a plane parallel to the reference image plane where the other images are projected. For each pixel $i$ in the reference images, it compares the neighborhood patch against the corresponding patches in the set of projected images, and finally choose the depth $z_i$ corresponding to the best matching score. 
Following approaches propose the usage of planes in multiple directions in the whole scene \cite{gallup2007real} or locally \cite{sinha2014efficient}
Space sweeping approaches compares the image with more accuracy, thanks to the 3D projection, but the computational effort is very huge, even if highly parallelizable \cite{yang2003multi}.

The previous approaches outputs a very noisy depth map that needs to be smoothed conveniently. 
A elegant approach to depth map estimation process aims at minimizing the energy:
\begin{equation}
 \label{eq:depthenergy} 
 E(z) = \sum_i \phi(z_i)  + \sum_{ij} \psi(z_i,z_j)
\end{equation}
that combines the function $\phi$ encoding the photo-consistency described in the previous approaches, and a function $\psi$ defined over the neighborhood of the pixel $i$ that penalizes differences to encode a smoothness prior. The minimization of this energy usually bootstrap from the noisy depth maps and leads to a more accurate and smooth set of depth maps.

Different optimization tools have been adopted to minimize \eqref{eq:depthenergy}. In \cite{campbell2008using} the authors stores multiple hypothesis for each pixel depth, estimated with a classical stereo matching algorithm, then they optimize this initial depth map with multiple label, modeling the problem as a Markov Random Field. Other optimization adopt graph-cuts \cite{kolmogorov2002multi} or Bayesian approaches \cite{strecha2006combined,gargallo2005bayesian} or propagation belief methods often referred as patch-based methods, since they usually estimate small oriented rectangular patches in 3D from seeded high accurate points to their neighborhood \cite{fu10,goesele2007multi,Tola12,bleyer2011patchmatch,heise2013pm}.




The estimated depth maps contains redundant information (the regions of overlap) sometimes still noisy, i.e., the 3D points corresponding to the estimated depths, often occupy overlapped regions, and their accuracy may be still far from the true values, for instance in the lack of texture regions.
Some recent works address explicitly to these issues. 
In \cite{semerjian2014new} the authors estimate the depth maps as an energy minimization problem that aims at produce a edge-preserving smooth depth maps.
In \cite{wei2014multi} a coarse-to-fine approach together with propagation of the depth belief lead to clean depth maps.

Another source of redundancy comes for instance where a region is flat and each of this planar region is modeled. Moreover a dense and continuous 3D model of the scene still needs to be reconstructed.

The subsequent step is therefore the fusion of the depth maps. This step requires a different representation of the data, so we will describe depth fusion in the following paragraphs, in which the input of the algorithms will often be a set of depth maps or a point clouds, estimated from the depth maps  as in \cite{curless1996volumetric} or directly through stereo matching techniques as in \cite{bradley2008accurate,labatut2007efficient,vu_et_al_2012}.


\subsection{Volumetric}
Many MVS algorithms model the scene from a point cloud, from a depth map or directly from images, by relying on a 3D discretization of the world space.
A sub-classification of these volumetric approaches considers how the space is discretized: with voxels or with tetrahedra.


\subsubsection{Voxel-based}
A voxel is a the 3D extension of a pixel: if we partition the space in a regular 3D grid, each part, i.e., each small cube, is a voxel.
Voxel-based  algorithms are popular volumetric approach to merge depth maps or directly model from image, and performs very well in the Middelbury dataset \cite{Seitz_et_al06}.

The early approach proposed in \cite{curless1996volumetric}, and applied in \cite{goesele2006multi}, cumulates depth maps data in order to estimate a signed distance of each voxel from the scene surface; the final reconstruction is defined as the zero-crossing surface on the voxels.
The authors in \cite{zach2007globally} estimate a more robust distance function through total variation regularization with a $L^1$ norm data fidelity term.

Another very common approach to create a model out of the 3D oriented point associated with the depth maps, is the Poisson reconstruction \cite{kazhdan2006poisson}, which computes an indicator function (defined as 1 at the points inside the model,and 0  outside) as a Poisson problem from a gradient field depending on the integral of the surface normal field.
An improvement on the Poisson Surface Reconstruction was recently proposed in \cite{shan2014occluding} where the internal contours estimated on the depth map, acts to densify the depth maps and, free space constraints computed for each voxels, constrain the Poisson reconstruction in order to handle free space volumes.

A classical approach not requiring the depth map estimation, labels the voxels as free space or occupied, and is known as space carving or voxel coloring \cite{seitz1999photorealistic,kutulakos_seitz05}. 
This method starts with a voxelized volume circumscribing the scene, and all voxels are initialized as occupied (or ``matter``). Low photo-consistent voxels are marked as free space iteratively, until a fixed photo-consistency threshold is reached. The boundary between free space and matter is therefore the final model of the scene. 
This method need to make hard decision every time a voxel needs to be classified, and highly depends on the order of inspection. Moreover, the output may results in a noisy reconstruction with artifacts and outliers.
An improvement was proposed in \cite{broadhurst2001probabilistic}, in which soft constraints are adopted to label more fairly the voxels.
Finally, the authors in \cite{yang2003multi} improve the order of inspection and adds a smoothing term. 
However the reconstruction of voxel-based space carving is still noisy and not  accurate.
Space Carving, was also applied, more successfully by tetrahedron-based volumetric approach we will  review in next section.

More successful approaches to estimate a surface without depth maps, relies on global optimization of the energy defined by:
\begin{equation}
\label{eq:eqVoxel}
E(f) = E_{\text{data}}(f) + E_{\text{smooth}} (f) + E_{\text{visibility}} (f)  
\end{equation}


where $f:\mathit{P}\leftarrow \mathit{L}$ is the labeling function that associates a label $l\in \mathit{L}$ to the pixel $p \in\mathit{P}$, $E_{\text{data}}(f)$, $E_{\text{smooth}} (f)$ and $E_{\text{visibility}} (f)$ are related respectively to the photo-metric cost, the smoothness prior and the visibility constraints.

The most common optimization has been accomplished via graph cuts \cite{vogiatzis2005multi,kolmogorov2002multi,hornung2006hierarchical,furukawa2006carved,mucke2011surface,hernandez2007probabilistic}: the voxels are nodes of the graph, while the edges connect neighboring pixels and their capacity depends on the energy \eqref{eq:eqVoxel}. The boundary voxels links with the sink node of the graph, and voxels that are very likely to be inside the surface links to the source.
These algorithms needs a ballooning term to avoid the shrinking bias, i.e., the minimization tends to traverse as few as possible edges and tends to extract an empty surface. If this term is too large, then the solution tends to over-inflate, otherwise the solution collapses into an empty surface.
This limitation has been partially solved in \cite{hernandez2007probabilistic} where the ballooning term depends  from the visibility, or in \cite{mucke2011surface} by replacing it with convenient weighting of the edges. 
Recently, convex optimization have been also successfully applied and reaches remarkable results
\cite{kolev2009continuous,kolev2010anisotropic,kostrikov2014probabilistic}, they formalize the labeling problem as in the previous case, but compute the labels woth the primal-dual method \cite{mehrotra1992implementation}.




In general, voxel-based methods yields very accurate results, in particular when coupled with global optimization methods; however, their application is still limited by the huge requirement of memory, and even if attempts to make the representation more compact exist \cite{steinbrucker2014volumetric,chen2013scalable,zeng2013octree}, they are still not suitable for scalable large-scale reconstruction.
Moreover, only by including shape priors these methods are able to handle lack of texture \cite{karimi2015segment} that usually affects the depth maps.


\todo{FIGURE}.
\subsubsection{Tetrahedron-based}
\label{sec:tet-based}
A different volumetric approach discretize the space with a set of tetrahedra, usually computed via Delaunay Triangulation on a point cloud.
These methods are popular in the Computer Graphics community to reconstruct a surface from an unorganized (no adjacent among points is known) set of point \cite{amenta1999surface,amenta2001power,boissonnat1984geometric,dey2004provable,kolluri2004spectral}. 

Fewer tetrahedron-based methods with respect to the voxel-based ones, have been adopted in multi-view stereo reconstruction
\cite{faugeras_et_al_90,labatut2007efficient,salman2010surface,vu_et_al_2012,hiep2009towards,Pan_et_al09}.

They often rely on the space carving algorithm similar to the voxel coloring, but the boundar between free space and occupied parts is naturally a triangular mesh, very well-suited for further mesh refinement algorithms.

In the seminal work of \cite{faugeras_et_al_90} they apply a constrained Delaunay 3D triangulation to fit the stereo edges. They label  as free space the tetrahedra crossed by visibility triangles from the camera to the edges as the space carving of the voxel-based approaches. The authors apply several times a region growing from not-labeled tetrahedra  until all not-labeled tetrahedra have been visited. Each iteration of each region growing keeps the boundary 2-manifold. Finally, each region is labeled as matter only if contains enough vertices, otherwise is free space. 
Even if this approach address the problem of reconstructing manifold surface, it does not handle genus greater than 0, e.g., a torus cannot be modeled, moreover the heuristic to label the tetrahedra is not robust.

Labatut \emph{et al.} \cite{labatut2007efficient} proposed to build the Delaunay triangulation upon a point cloud rich of points after the removal of redundant points. 
They extract the mesh of the model with the graph cut algorithm: they minimize an energy that takes into account photo-consistency and visibility. The results are scalable and accurate, but the final reconstruction is not guaranteed to be 2-manifold.
In \cite{hiep2009towards} and \cite{vu_et_al_2012} the authors apply a similar approach but they subsequently optimize the mesh with a mesh evolution algorithm

ProForma \cite{Pan_et_al09} is an online reconstruction algorithm that tracks 2D features on the image frame by frame; their reconstruction performed every $k$ frames is added in a 3D Delaunay triangulation. They reconstruct a mesh through a probabilistic space carving approach. The approache works online for small objects, but scales poorly, since the triangulation and the reconstruction are executed for each keyframe.

In general, tetrahedron-based algorithms are volumetric methods  much more compact than voxel-based ones, but the accuracy of the reconstruction still needs a refinement to reach state-of-the-art results, indeed the output surface mesh is used as an initialization to mesh-based methods in \cite{vu_et_al_2012,hiep2009towards,salman2010surface}.
\todo{FIGURE}.
\subsection{Level set}
Level set methods define a model of the scene as the zero crossing of a function $f:\mathbf{R}^3\leftarrow\mathbf{R}$ \cite{faugeras2002variational,jin2002variational,yezzi2003stereoscopic,fuhrmann2014floating,solem2005geometric,yoon2010joint,pons2007multi}. 
Usually the bootstrap the reconstruction from an initial surface and they evolve it by a variational approach which aims at minimize an energy function.

For instance, in \cite{faugeras2002variational} the evolution of the surface, defined as zero crossing of $f$, is described by partial differential equation of the function $f$. A point on the surface moves proportionally to a value depending on the photo-consistency of its neighborhood: in particular, the more the neighbors are photoconsistent, the ore the point is near to zero.
The work in \cite{jin2002variational} extends this approach by managing surfaces with specular reflection thanks to a convenient choice of the pairwise cameras involved in the photo-consistency computation. In \cite{yoon2010joint} both the reflectiveness and the shape of the scene eas estimated in the variational framework.

In both approaches the photometric measure is integrated in the 3D domain. In \cite{yezzi2003stereoscopic} the authors shows how the integration of this measure on the image domain produce more accurate results.

Solem \emph{et al.} \cite{solem2005geometric} propose a geometric formalization of the variational minimization, in particular replacing the partial differential equation with the gradient, showing its robustness.
In \cite{pons2005modelling} non rigid scene were addressed, improved in \cite{pons2007multi}  with a new variational framework that takes into account a global matching score, instead of the local presented by the previous approaches.

A recent work of Fuhrmann \emph{et al.} proposes a new floating scale implicit function which integrates in the reconstruction  points together with their scale, e.g., patch size, which may differ from one point to another. 


Level set methods lead to accurate results, handles implicitly topology changes, but often relies on a memory consumption volumetric initialization, and the evolution of the surface is not always easy to track and understand.
\subsection{Mesh}

The last class of MVS algorithms represents the scene as a surface mesh. Some of the methods of the previous paragraphs outputs a mesh, but is always estimated from a different representation which play the major role in the reconstruction process.

Instead the following methods deals directly with a mesh recovered from point clouds, depth maps or as a evolution of an initial mesh surface.

The early Mesh Zippering algorithm \cite{turk1994zippered} creates a triangular mesh from depth maps. It registers all the point clouds with a variant of the ICP (Iterative Closest Point); it creates a mesh for each depth map and finally merge all the meshes.

Mesh from depth images are directly estimated in \cite{pollefeys_et_al_08} too, by applying a coarse-to-fine approach for each depth camera: they bootstraps from a 3D rectangle mesh made up by two triangles that covers all the depth image, each triangle is subdivided and its position re-estimated whenever discontinuity or non-planarity in the depth map region enclosed in the triangle occurs. The meshes generated for each depth maps are registered thanks to the sensor calibration, and the redundant triangles are deleted.

Finally, Bradley \emph{et al.} \cite{bradley2008accurate} propose to triangulate clusters of the point cloud estimated via depth maps, estimate a mesh out of the triangulation and merge the sub maps created.

Generally, these algorithms estimates a mesh for each depth maps and merge the surfaces mesh to obtain a final model.
A more direct approach that avoids the mesh merging procedure, which may lead to artifacts, other mesh-based algorithms rely on the volumetric approaches presented in \ref{sec:tet-based}.

A deeply different class of mesh-based algorithms aims at refine a coarse mesh of the scene, for instance the visual hull \cite{hiep2009towards,zaharescu2007transformesh,delaunoy_et_al_08,gargallo2007minimizing,delaunoy2011gradient,vu2011large}. These algorithms are similar to the level-set method but directly evolve the mesh in 3D, without the need to extract the surface from an implicit function representation.

The methods proposed in \cite{delaunoy_et_al_08,gargallo2007minimizing,delaunoy2011gradient} evolve the surface to minimize the energy \eqref{eq:eqVoxel} rewriting the photo-consistency term for an image $I$ of camera $i$ as:
\begin{equation}
\label{eq:generative}
  E_{\text{data}}(S) = \int_{\mathit{I}} \rho\left(I(\mathit{u}), I^*_{\Sigma}(\mathit{u})\right) d\mathit{u}
\end{equation},
where $\rho$ is a photo-consistency measure, $I$ is the image while $I^*$ is the current estimated surface and texture, rendered in camera $i$.
This equation integrates the photo-consistency in the image domain while most of other level set methods integrate this measure in the surface domain. This lead to more natural Bayesian formalization of the reconstruction. 
However, these approaches, named generative, have the drawback of not implicitly considering the occlusions which makes the functional difficult to be optimized. They need an explicit management of the visibility as in \cite{delaunoy2011gradient}.

A more straightforward approach to surface evolution was proposed by \cite{hiep2009towards} and \cite{vu2011large} which minimize an energy between pairwise images $i$ and $j$:
\begin{equation}
\label{eq:nongen}
  E_{\text{data}}(S) = \int_{\Sigma^{\mathit{S}}_{ij}} \rho\left(I(\mathit{u}), I^{\mathit{S}}_{ij}(\mathit{u})\right) d\mathit{u}
\end{equation}
where the image $I^{\mathit{S}}_{ij}$ is the backprojection of the image of camera $j$ to camera $i$ through the current estimated surface $\mathit{S}$, i.e., $I^{\mathit{S}}_{ij} = I_j \circ \Pi_j \circ \Pi_i^{-1}$ , $\Sigma^{\mathit{S}}_{ij}$ represent the domain of the surface where the reprojection is defined.

Both the approaches optimize the energies \eqref{eq:generative} and \eqref{eq:nongen} via gradient descent; let $\mathit{S}^0$ be the initial surface and $\mathit{S}(t)$ the surface evolved at time $0$, then:
\begin{equation}
 \begin{cases}
  \mathit{S}(0) &=\mathit{S}^0\\
  \frac{\partial \mathit{S}(t)}{\partial t} & = -\bigtriangledown_M E_{\text{data}}(S)
 \end{cases}
\end{equation}


These mesh-based methods haveproved to lead to very accurate results and  by estimating  sub-maps and then merge them together as in \cite{vu2011large},  large-scale reconstruction is feasible both on computational and memory sides.
\todo{FIGURE}.

\subsection{Initialization}
\subsection{Shape priors}
\subsection{Large Scale}
\section{other sensors}


\section{Incremental reconstruction}
\subsection{Incremental Structure from Motion and Simultaneous Localization and Mapping}

\subsection{Incremental reconstruction from sparse points}
Incremental 3D reconstruction from a sparse point cloud is gaining interest in the computer vision community as incremental Structure from Motion algorithms are consolidating  \cite{wu13}. 
This is clearly true for those applications where a rough, but dense, surface represents a sufficient and effective representation of the scene, e.g, for traversability analysis in unmanned vehicle navigation. 
Furthermore, in real-time applications, the map of the environment needs to be updated online, and the surface has to be estimated incrementally. 

Most of the existing incremental algorithms \cite{Lovi_et_al_11,Pan_et_al09,litvinov_lhuillier_13,litvinov_Lhiuller14} bootstrap the reconstruction of a mesh surface from the 3D Delaunay triangulation of a sparse point cloud. Indeed, the Delaunay property, i.e., no point of the triangulation is inside the sphere circumscribing any tetrahedron, avoids as much as possible the resulting tetrahedra to have a degenerate shape \cite{Maur_02}; it is self-adaptive, i.e., the more the points are dense the more the tetrahedra are small; it is very fast to compute, and to  update against point removal or addition; off-the-shelf libraries, such as CGAL \cite{cgal}, enable a very simple and efficient management of it. 

As soon as a Delaunay triangulation is available, several approaches exist to extract a surface taking into account the visibility of each point. 
The simplest algorithm is the Space Carving \cite{kutulakos_seitz05}: it initializes all the tetrahedra as \emph{matter}, then it marks as \emph{free space} the tetrahedra intersected by the camera-to-point \emph{viewing rays}, i.e., the lines from the camera center to the observed 3D points in the triangulation. 
The boundary between free space and matter represents the final surface of the scene.
Pan et al. \cite{Pan_et_al09} improve upon this simple procedure by proposing an online probabilistic Space Carving, but this is not an incremental approach: they start from scratch every time new points are added.
Lovi et al. \cite{Lovi_et_al_11} present the first incremental Space Carving algorithm which runs real-time, but, as for the previous methods, the estimated surface is not guaranteed to be manifold 

Several reasons lead to enforce the manifold property as explained in \cite{lhuillier20152}. 
Most Computer Graphics algorithms need the manifold property, for instance smoothing with Laplace-Beltrami operator \cite{Meyer03}, or the linear mesh parametrization \cite{saboret00}.
Moreover the manifold property enables surface evolution in mesh-based Multi-View Stereo, as in \cite{vu_et_al_2012,delaunoy_et_al_08}.the manifold property enables a photometric refinement by surface evolution such as with the high accurate Multi-View Stereo mesh-based algorithm as in \cite{vu_et_al_2012,delaunoy_et_al_08}.
With these approaches is hard to estimate the surface evolving flow in the presence of non manifold vertices: indeed they compute for each vertex the gradient minimizing the reprojection error, by summing-up the contribution of the incident facets; if the vertex is not manifold, this gradient does not converge. As a further proof of this, \cite{vu_et_al_2012} needs to manually fix the surface estimated via s-t cut.
As in \cite{vu_et_al_2012}, it is possible to fix the mesh as a post-processing step, but reconstructing directly a manifold as in the proposed paper, enables the design of a fully automatic pipeline which do not need human intervention.

In literature, the only algorithm reconstructing a manifold incrementally was proposed by Litvinov and Lhuiller \cite{litvinov_lhuillier_13,litvinov_Lhiuller14}. 
In their work, the authors  bootstrap from the Space Carving procedure and, by taking into account the number of intersections of each tetrahedron with the viewing rays, they reconstruct a surface keeping the manifold property valid. 
The main limitation is that  Litvinov and Lhuiller insert a point into the Delaunay triangulation only when its position is definitive, then they cannot move the point position anymore even in the case they could refine their estimate. 
The main reason of Litvinov and Lhuiller design choice has to be ascribed to the computational cost of updating the visibility information along the viewing rays incident to each moved point, and the computational cost of updating part of the Delaunay triangulation, which in turn induces a new manifold reconstruction iteration step.

Indeed, the very common approach to deal with a point moving in the triangulation, is to remove it and add it back in the new position \cite{cgal} (Fig. \ref{fig:moving}). 
When we remove a point (the point A in Fig. \ref{fig:moving}(a)) and we want to keep the Delaunay property, we have to remove all the tetrahedra incident to that point (light red triangles in Fig. \ref{fig:moving}(b)); then, we add a new set of tetrahedra to triangulate the resulting hole (dark green triangles in \ref{fig:moving}(c)).
When we add a new point into the triangulation  (the point B in Fig. \ref{fig:moving}(d)), a set of tetrahedra would conflict with it, i.e., the Delaunay property is broken (light red triangles in Fig. \ref{fig:moving}(d)); so, we remove this set of tetrahedra again (red triangles in Fig. \ref{fig:moving}(e)) and we add a new connected set that re-triangulate the hole (dark green triangles in Fig. \ref{fig:moving}(f)).
Whenever a set of tetrahedra is replaced, we have to transfer conveniently the information about the visibility (matter or free space) of the removed tetrahedra to the new one. 
In addition to this, we have to update the visibility of the tetrahedra crossed by a visibility ray from one camera to the moved point.
For these reasons the update of the point position is computational demanding.

To complete the overview of the incremental reconstruction methods from sparse data, we mention here another very different approach was proposed by Hoppe et al. \cite{Hoppe13} who label the tetrahedra with a random field, and extract the surface via graph-cuts by minimizing a visibility-consistent energy function. This incremental algorithm is effective and handles the moving points, but the manifold property of the reconstructed surface is not yet guaranteed.



\section{Thesis contributions}

