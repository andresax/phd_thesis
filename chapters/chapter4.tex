\chapter{From Batch to Incremental 3D Reconstruction}

3D Reconstruction represents a long-standing topic of interest in both Computer Vision and Robotics communities. 
While in Computer Vision the main goal is to increase the accuracy and the details of the reconstructed model, leveraging on computationally intensive optimization methods, Robotics aims at rapidly understand how the circumscribing world looks like, looking for the trade-off between accuracy and computational feasibility.
In recent years, the advances in the hardware capability, especially related to the powerful commodity graphics hardware (GPU-processing), and thanks to the design of efficient optimization algorithms, led Robotics and Computer Vision to become closer and closer in the 3D reconstruction (or mapping) field.
This chapter will give a comprehensive overview of both batch and incremental approaches to reconstruct the scene from a set or a sequence of images. We will focus in particular in the complex literature of Multi-View Stereo, to have a better understanding of some principles that are the basis of this thesis that aims at filling the gap between Structure from Motion/Simultaneous Localization and Mapping world and Multi-View Stereo itself. 

\minitoc

% The reconstruction in computer vision aims at estimating an as accurate as possible map of the environment


\section{Sparse Scene Structure Reconstruction and Camera Pose estimation}
\label{sec:slam}
Both Structure from Motion (SfM) in Computer Vision and Simultaneous Localization and Mapping (SLAM) in Robotics face the problem of reconstructing a sparse map of the environment together with the pose (position and orientation) of the camera capturing the scene.
The early researches of the two communities, focused on different aspects of such problem.
SfM aims in particular at  estimating accurately the map (structure) of a generic set of images, it had no particular time constraints and processed all the images in the same time (batch).
On the other hand, SLAM was thought to be deployed on a robot, therefore, the main goal of is to obtain a fast and accurate estimate of the robot pose with respect to the environment by processing a video sequence: the SLAM algorithms needs to perform in real-time, \ie, at the same rate of the camera, and incrementally, \ie, a new image  has to be included in the estimation as soon as it becomes available.


These different approaches to the same problem, led the researchers of the two communities to develop deeply different algorithm relying on different estimation tools.
Classical Structure from Motion algorithms \cite{triggs2000bundle,sibley2009adaptive,wu2011multicore} extract a set of 2D points for each image with a descriptor associated, such as SIFT \cite{sift} or ORB \cite{orb}, or others, then they find the correspondences of these 2D point among all the images that are generated by the same 3D point. Bootstrapping from an initial guess, they globally optimize the pose of the camera and the 3D points position, by minimizing the error between the 2D points (the measurements) and the the projection of the current estimate of the 3D points on the cameras.
If we deal with $n$ cameras, $k$ 3D points, we express $x_{ij}^{2D}$ the measurement of point $i$ in image $j$, and $\Pi_j(x_i^{3D})$ projects the 3D point $i$ on the image plane of the $j$-th camera, SfM aims at minimizing the following:
\[
\sum_{k}^{i=0}\sum_{n}^{j=0}||x_{ij}^{2D} - \Pi_j(x_i^{3D})||^2.
\]
It is usually minimized by Gauss-Newton or Levenberg-Marquardt algorithms, and the process is usually named as Bundle Adjustment, since the minimization process adjusts the bundles of camera-to-point, by moving the camera poses and 3D point positions. This estimation usually requires a big computational effort.


Instead, the SLAM approach \cite{davison2007monoslam,ceriani2014single,grasa2011ekf}, classically adopts a different point of view, focusing on the efficiency of the estimate, and so, it uses a different estimation tool, \ie, the Extended Kalman Filter (EKF) which estimates iteratively a state, composed by the camera pose with respect to the world together with the map of the environment, by incorporating the new measurement and marginalizing the old camera poses.  


Thanks to the Parallel Tracking and Mapping (PTAM) algorithm, proposed in \cite{klein_murray07}, these two approaches become closer and closer. PTAM proposed a SLAM system that adopts the bundle adjustment optimization: it decouples the fast tracking process, \ie, the camera pose estimation, and the slow mapping processes in two parallel thread.  
The tracking processes each frame of the video sequence while the mapping works only on keyframes. 
This new paradigm, named keyframe-SLAM, gain more and more interest in the robotics community, such that it become more popular with respect to the filtering approach \cite{strasdat2010real}.
Moreover, the improvements on the bundle adjustment optimization process proposed in \cite{kaess2008isam},  the formalization of the SLAM problem as a factor graph\cite{thrun2006graph}, and the optimization libraries g2o \cite{kummerle2011g} and GTSAM \cite{dellaert2012factor} led the researchers to move from the EKF estimation to keyframe slam based on bundle adjustment \cite{strasdat11,sunderhauf2012towards,johannsson2013temporally}. 

% LSD-SLAM

In conclusion both SfM and SLAM algorithms are able to estimate the pose of the cameras and a point cloud representing the map of the environment: the former handles a generic set of data while the latter deals with a video sequence. 
In this thesis we rely on these points and cameras estimation and we provide a system able to handle a video sequence, or a set of subsequent images, that build a very accurate, continuous and dense mesh map of the environment, exploiting the advances in Multi-View Stereo and incremental reconstruction from sparse points.

\section{Multi-View Stereo}


A wide literature in Computer Vision focuses on reconstructing a scene directly from the images by assuming that the camera poses are known, usually estimated by a Structure from Motion algorithm.
These algorithms, known as Multi-View Stereo (MVS), aims at recovering a very detailed dense and accurate reconstruction. 
While, SfM and SLAM algorithms previously described output a sparse point cloud as a representation of the map of the environment, MVS aims at reconstructing at least the big majority of the image pixels, resulting in a very dense model of the environment.

\subsection{Tools and Concepts}
Before a review of existing approaches  in Multi-View Stereo, we present some concepts that are common in almost all algorithms.
\subsubsection{Photoconsistency}
Multi-View Stereo aims at recovering 3D information from a set of calibrated images. The coherence and accuracy measure to evaluate and optimize the recovered 3D data is called photo-consistency, which measures the similarity between two image regions.
Different photo-consistency measure have been used. 
Given images $I$ and $J$ and two pixels $p_0\in I$ and $q_0\in J$. Very simple measures rely on the difference between the intensity values  $I(p_0)$ and $J(q_0)$ but, taking into account only a single value, they lack in robustness. 
Robust photo-consistency measures consider a set of neighboring  pixels around $p_0$ and $q_0$, \ie, $p_i$ and $q_j$ where $1\leq i \leq n$ and $1\leq j \leq n$. Usually the neighboring pixels form a rectangular region; common measures are:
\begin{itemize}
  \item Sum of Squared Differences (SSD): $\sum_{i=0}^{n}(I(p_i) - J(q_i))^2$
  \item Sum of Absolute Differences (SAD): $\sum_{i=0}^{n}|I(p_i) - J(q_i)|$
  \item Normalized Cross-Correlation (NCC): $\frac{v_{I,J}(p_0,q_0)}{\sqrt{v_{I}(p_0} v_{J}(q_0}$,
  where 
  \begin{itemize}
    \item $v_{IJ}(p_0,q_0) = \frac{\sum_{i=0}^{n}I(p_i)J(q_i)}{n} - \mu_I(p_0) \mu_{J}(q_0)$
    \item $\mu_{I}(p_0) = \frac{\sum_{i=0}^{n}I(p_i)}{n}$ and $\mu_{J}(q_0) = \frac{\sum_{i=0}^{n}J(q_i)}{n}$ are the two mean values of the pixels considered in image $I$ and $J$
    \item $v_{I}(p_0) = \frac{\sum_{i=0}^{n}I(p_i)^2}{n} - \mu_I(p_0)^2$ and $v_{J}(q_0) = \frac{\sum_{i=0}^{n}J(q_i)^2}{n} - \mu_{J}(q_0)^2$ are the variances
  \end{itemize}
\end{itemize}
Low values of SSD and SAD represents high photo-consistency, while NCC is the correlation between two image regions. 
The computation of the NCC measure is more complex , but the final score is robust against linear illumination changes.


\subsubsection{Variational methods}
\label{subsec:variational}
In literature variational methods have been extensively adopted to reconstruct an accurate and photo-consistent model of the scene.
 They are widely used in the Computer Vision community, to solve an optimization problem involving an energy minimization.
Variational methods, also named  calculus of variations, aims at minimizing (or maximizing) a functional, \ie, a mapping $E$ from a vector space (each function $u$) to a real number. 
An example of general functional can be written as:
\begin{equation*}
 E(u) = \int \mathit{L} (u, u')dx
\end{equation*}
with $u'=\frac{du}{dx}$.
To minimize the functional $E$ we need to find its derivative with respect to the function $u$, through the Euler-Lagrange equation:
\begin{equation*}
 \frac{\partial}{\partial u} \mathit{L} (u, u') - \frac{d}{dx} \frac{\partial}{\partial u'}\mathit{L} (u, u') =0
\end{equation*}
The variational framework can be applied in Multi-View Stereo reconstruction \cite{hermosillo2002variational} as follows.

Given a set of images $\mathit{I}$ the functional $E$ usually writes as:
\begin{equation}
\label{eq:variational}
E(\mathit{u}) = E_{\text{data}}(\mathit{u}, \mathit{I}) + E_{\text{reg}} (\mathit{u})  
\end{equation}
where $E_{\text{data}}$  and $E_{\text{reg}}$ represent the data and the regularization terms, and function $\mathit{u}$ depends on the representation adopted to represent the 3D model of the scene.
For instance, in the surface evolution setting the function $\mathit{u}$ becomes the surface $\mathit{S}$ and the goal is to find the surface that minimizes the energy \eqref{eq:variational} usually written as:
\begin{equation*}
 E(\mathit{S}) = \int_{\mathit{S}} g(\mathbf{x}, \mathbf{n}_{\mathit{S}}) \quad ds  +\int_{\mathit{\Omega}} f(\mathbf{x}) \quad d\mathbf{x}
\end{equation*}
The function $\mathit{g}:\mathbb{R}^3\times\mathbb{S}^2 \rightarrow \mathbb{R}^+$ is named \emph{weighted area functional}  defined on the surface $\mathit{S}$ and represents the photo-consistency contribution to the total energy at point $\mathbf{x}$ with the associated normal $\mathbf{n}_{\mathit{S}}$.
The function $\mathit{f}$ is the so called \emph{ballooning} term and acts in the interior $\mathit{\Omega}$ of the surface $\mathit{S}$ as a potential that avoids the shrinkage to an empty surface (known as \emph{minimal surface bias}).

Gradient descent is a classical approach to minimize this energy, therefore Euler-Lagrange equation has to be applied in order to compute the gradient $\bigtriangledown E(\mathit{S})$ \cite{hermosillo2002variational}. Let $\mathit{S}^0$ be the initial surface and $\mathit{S}(t)$ the surface evolved at time $0$, then:
\begin{equation}
 \begin{cases}
  \mathit{S}(0) &=\mathit{S}^0\\
  \frac{\partial \mathit{S}(t)}{\partial t} & = -\bigtriangledown E(\mathit{S})
 \end{cases}
\end{equation}
Usually the two terms $E_{\text{data}}$ and $E_{\text{reg}}$ are threated separately since their meaning is deeply different.
The data term strictly depends on the representation of the model and the photo-consistency measure adopted, we will discuss about this term in the section about mesh-based algorithms, since the thesis addresses to such representation.

\subsubsection{Regularization}
In Multi-View Stereo, the reasons to enforce regularization are twofold. From a formal perspective, surface reconstruction can be described in a Bayesian point of view \cite[11]{delaunoy2011modelisation}: the photo-consistency term represents only what we can infer from the observations (the images), instead the regularization represents the prior knowledge about the surface, \ie, the surface has to be smooth and regular. 
From a more practical point of view, images contains noise and the regularization term reduces the noise transferred in the reconstructed surface and it makes the optimization process more robust, \eg, by avoiding drifting degeneracies.

In the variational framework described thus far, regularization is achieved by minimizing the term $E_{\text{reg}}$. 
A common choice of this term lead the optimization to minimize the total area the surface, in order to obtain smooth solution:
\[
E_{\text{reg}} = \int_{\mathit{S}} ds.
\]
This bias the optimization to the minimal surface, and to avoid the empty surface solution the ballooning term described in previous section is adopted.

A more robust approach regularize the surface normals, by minimizing the following energy:
\[
E_{\text{reg}} = \int_{\mathit{S}} |\mathbf{n}(\mathbf{x}) - \mathbf{h}(\mathbf{x})|^2 ds
\]
where $\mathbf{n}(\mathbf{x})$ is the normal at point $\mathbf{x}$ and $\mathbf{h}(\mathbf{x})$ is the mean normal of the neighboring points of $\mathbf{x}$.

% \cite{li2015detail,heise2013pm,graber2015efficient}

A different approach to regularization relies on the Total Variation of the surface \cite{chambolle2010introduction}, \ie, 
\[
\int_{\mathit{\Omega}}|\bigtriangledown \mathit{S}|d\mathbf{x}
\];
we recall that  $\mathit{\Omega}$ is the interior of the surface $\mathit{S}$, and thanks to the absolute value operator, the functional is convex, therefore convex optimization methods may be applied.



\subsection{Shape Representations}
Since the datasets of \cite{Seitz_et_al06} and \cite{strecha2008} were made available, dense MVS has been faced with different approaches, some of which reach very accurate results on these datasets.
It is hard to provide a unique classification of MVS algorithms since the methods proposed may differ in many aspect, such as the input assumptions, the algorithm to obtain the reconstruction, optimization methods. Following the taxonomy proposed in \cite{Seitz_et_al06}, we classify the MVS algorithms according to the representation of the model: \emph{depth maps}, \emph{volumetric},  \emph{level set} and \emph{mesh} methods. The four classes does not exist alone, by themselves, but a Multi-View Stereo algorithm, especially the most complex and recent ones, often propose a pipeline requiring different representations of the scene. In particular the depth maps-based algorithm usually needs a meshing step achieved by volumetric or implicit surfaces methods (level set).

\subsubsection{Depth Maps}
A depth map is a particular image that stores for each pixel the depth of the scene.
Usually the approaches based on this representation involve two steps: reference depth maps estimation and map merging.
Given a set of images, for each pixel $i$ of a reference image $I_{\text{ref}}$, the depth estimation aims at recovering the pixel depth, by pairwise comparing the appearance of the images in a process also known as stereo matching \cite{scharstein2002taxonomy}. 
The basic idea is to look for the most likely depth $z_i$ generated from the $i$-th pixel as depicted in Figure \todo{FIGURE}.

A very popular approach compares two images as follows: for each pixel $i$ of the reference frame, it scans the corresponding epipolar line in the second image, and looks for the pixel whose neighboring patch best matches the patch around $i$  \cite{lhuillier2002match}. 
Commonly used matching costs are the SSD (Sum of Squared Differences) or the NCC (Normalized Cross Correlation).
Some algorithms first rectify the two compared images, such that the epipolar line becomes horizontal and the scanning process becomes easier 
\cite{kang2001handling,bradley2008accurate,moons20093d}. 

Another approach named plane-sweeping \cite{collins1996space}, scans the entire 3D scene, that needs to be discretized, with a fronto-parallel plane, \ie, a plane parallel to the reference image plane where the other images are projected. For each pixel $i$ in the reference images, it compares the neighborhood patch against the corresponding patches in the set of projected images, and finally choose the depth $z_i$ corresponding to the best matching score. 
Following approaches propose the usage of planes in multiple directions in the whole scene \cite{gallup2007real} or locally \cite{sinha2014efficient}
Space sweeping approaches compares the image with more accuracy, thanks to the 3D projection, but the computational effort is very huge, even if highly parallelizable \cite{yang2003multi}.

The previous approaches outputs a very noisy depth map that needs to be smoothed conveniently. 
A elegant approach to depth map estimation process aims at minimizing the energy:
\begin{equation}
 \label{eq:depthenergy} 
 E(z) = \sum_i \phi(z_i)  + \sum_{ij} \psi(z_i,z_j)
\end{equation}
that combines the function $\phi$ encoding the photo-consistency described in the previous approaches, and a function $\psi$ defined over the neighborhood of the pixel $i$ that penalizes differences to encode a smoothness prior. The minimization of this energy usually bootstrap from the noisy depth maps and leads to a more accurate and smooth set of depth maps.

Different optimization tools have been adopted to minimize \eqref{eq:depthenergy}. In \cite{campbell2008using} the authors stores multiple hypothesis for each pixel depth, estimated with a classical stereo matching algorithm, then they optimize this initial depth map with multiple label, modeling the problem as a Markov Random Field. Other optimization adopt graph-cuts \cite{kolmogorov2002multi} or Bayesian approaches \cite{strecha2006combined,gargallo2005bayesian} or propagation belief methods often referred as patch-based methods, since they usually estimate small oriented rectangular patches in 3D from seeded high accurate points to their neighborhood \cite{fu10,goesele2007multi,Tola12,bleyer2011patchmatch,heise2013pm}.




The estimated depth maps contains redundant information (the regions of overlap) sometimes still noisy, \ie, the 3D points corresponding to the estimated depths, often occupy overlapped regions, and their accuracy may be still far from the true values, for instance in the lack of texture regions.
Some recent works address explicitly to these issues. 
In \cite{semerjian2014new} the authors estimate the depth maps as an energy minimization problem that aims at produce a edge-preserving smooth depth maps.
In \cite{wei2014multi} a coarse-to-fine approach together with propagation of the depth belief lead to clean depth maps.

Another source of redundancy comes for instance where a region is flat and each of this planar region is modeled. Moreover a dense and continuous 3D model of the scene still needs to be reconstructed.

The subsequent step is therefore the fusion of the depth maps. This step requires a different representation of the data, so we will describe depth fusion in the following paragraphs, in which the input of the algorithms will often be a set of depth maps or a point clouds, estimated from the depth maps  as in \cite{curless1996volumetric} or directly through stereo matching techniques as in \cite{bradley2008accurate,labatut2007efficient,vu_et_al_2012}.


\subsubsection{Volumetric}
Many MVS algorithms model the scene from a point cloud, from a depth map or directly from images, by relying on a 3D discretization of the world space.
A sub-classification of these volumetric approaches considers how the space is discretized: with voxels or with tetrahedra.


\paragraph{Voxel-based}
A voxel is a the 3D extension of a pixel: if we partition the space in a regular 3D grid, each part, \ie, each small cube, is a voxel.
Voxel-based  algorithms are popular volumetric approach to merge depth maps or directly model from image, and performs very well in the Middelbury dataset \cite{Seitz_et_al06}.

The early approach proposed in \cite{curless1996volumetric}, and applied in \cite{goesele2006multi}, cumulates depth maps data in order to estimate a signed distance of each voxel from the scene surface; the final reconstruction is defined as the zero-crossing surface on the voxels.
The authors in \cite{zach2007globally} estimate a more robust distance function through total variation regularization with a $L^1$ norm data fidelity term.

Another very common approach to create a model out of the 3D oriented point associated with the depth maps, is the Poisson reconstruction \cite{kazhdan2006poisson}, which computes an indicator function (defined as 1 at the points inside the model,and 0  outside) as a Poisson problem from a gradient field depending on the integral of the surface normal field.
An improvement on the Poisson Surface Reconstruction was recently proposed in \cite{shan2014occluding} where the internal contours estimated on the depth map, acts to densify the depth maps and, free space constraints computed for each voxels, constrain the Poisson reconstruction in order to handle free space volumes.

A classical approach not requiring the depth map estimation, labels the voxels as free space or occupied, and is known as space carving or voxel coloring \cite{seitz1999photorealistic,kutulakos_seitz05}. 
This method starts with a voxelized volume circumscribing the scene, and all voxels are initialized as occupied (or ``matter``). Low photo-consistent voxels are marked as free space iteratively, until a fixed photo-consistency threshold is reached. The boundary between free space and matter is therefore the final model of the scene. 
This method need to make hard decision every time a voxel needs to be classified, and highly depends on the order of inspection. Moreover, the output may results in a noisy reconstruction with artifacts and outliers.
An improvement was proposed in \cite{broadhurst2001probabilistic}, in which soft constraints are adopted to label more fairly the voxels.
Finally, the authors in \cite{yang2003multi} improve the order of inspection and adds a smoothing term. 
However the reconstruction of voxel-based space carving is still noisy and not  accurate.
Space Carving, was also applied, more successfully by tetrahedron-based volumetric approach we will  review in next section.

More successful approaches to estimate a surface without depth maps, relies on global optimization of the energy \eqref{eq:variational} rewritten as:
\begin{equation}
\label{eq:eqVoxel}
E(\mathit{l}) = E_{\text{data}}(\mathit{l}) + E_{\text{reg}} (\mathit{l})  
\end{equation}


where $\mathit{l}:\mathit{P}\leftarrow \mathit{L}$ is the labeling function that associates a label $q\in \mathit{L}$ to the pixel $p \in\mathit{P}$, $E_{\text{data}}(\mathit{l})$ and $E_{\text{reg}} (\mathit{l})$ are related respectively to the photo-metric cost and the smoothness prior.

The most common optimization has been accomplished via graph cuts \cite{vogiatzis2005multi,kolmogorov2002multi,hornung2006hierarchical,furukawa2006carved,mucke2011surface,hernandez2007probabilistic}: the voxels are nodes of the graph, while the edges connect neighboring pixels and their capacity depends on the energy \eqref{eq:eqVoxel}. The boundary voxels links with the sink node of the graph, and voxels that are very likely to be inside the surface links to the source.
These algorithms needs a ballooning term to avoid the shrinking bias, \ie, the minimization tends to traverse as few as possible edges and tends to extract an empty surface. If this term is too large, then the solution tends to over-inflate, otherwise the solution collapses into an empty surface.
This limitation has been partially solved in \cite{hernandez2007probabilistic} where the ballooning term depends  from the visibility, or in \cite{mucke2011surface} by replacing it with convenient weighting of the edges. 
Recently, convex optimization have been also successfully applied and reaches remarkable results
\cite{kolev2009continuous,kolev2010anisotropic,kostrikov2014probabilistic}, they formalize the labeling problem as in the previous case, but compute the labels woth the primal-dual method \cite{mehrotra1992implementation}.




In general, voxel-based methods yields very accurate results, in particular when coupled with global optimization methods; however, their application is still limited by the huge requirement of memory, and even if attempts to make the representation more compact exist \cite{steinbrucker2014volumetric,chen2013scalable,zeng2013octree}, they are still not suitable for scalable large-scale reconstruction.
Moreover, only by including shape priors these methods are able to handle lack of texture \cite{karimi2015segment} that usually affects the depth maps.


\todo{FIGURE}.
\paragraph{Tetrahedron-based}
\label{sec:tet-based}
A different volumetric approach discretize the space with a set of tetrahedra, usually computed via Delaunay Triangulation on a point cloud.
These methods are popular in the Computer Graphics community to reconstruct a surface from an unorganized (no adjacent among points is known) set of point \cite{amenta1999surface,amenta2001power,boissonnat1984geometric,dey2004provable,kolluri2004spectral}. 

Fewer tetrahedron-based methods with respect to the voxel-based ones, have been adopted in Multi-View Stereo reconstruction
\cite{faugeras_et_al_90,labatut2007efficient,salman2010surface,vu_et_al_2012,hiep2009towards,Pan_et_al09}.

They often rely on the space carving algorithm similar to the voxel coloring, but the boundar between free space and occupied parts is naturally a triangular mesh, very well-suited for further mesh refinement algorithms.

In the seminal work of \cite{faugeras_et_al_90} they apply a constrained Delaunay 3D triangulation to fit the stereo edges. They label  as free space the tetrahedra crossed by visibility triangles from the camera to the edges as the space carving of the voxel-based approaches. The authors apply several times a region growing from not-labeled tetrahedra  until all not-labeled tetrahedra have been visited. Each iteration of each region growing keeps the boundary 2-manifold. Finally, each region is labeled as matter only if contains enough vertices, otherwise is free space. 
Even if this approach address the problem of reconstructing manifold surface, it does not handle genus greater than 0, \eg, a torus cannot be modeled, moreover the heuristic to label the tetrahedra is not robust.

Labatut \etal \cite{labatut2007efficient} proposed to build the Delaunay triangulation upon a point cloud rich of points after the removal of redundant points. 
They extract the mesh of the model with the graph cut algorithm: they minimize an energy that takes into account photo-consistency and visibility. The results are scalable and accurate, but the final reconstruction is not guaranteed to be 2-manifold.
In \cite{hiep2009towards} and \cite{vu_et_al_2012} the authors apply a similar approach but they subsequently optimize the mesh with a mesh evolution algorithm

ProForma \cite{Pan_et_al09} is an online reconstruction algorithm that tracks 2D features on the image frame by frame; their reconstruction performed every $k$ frames is added in a 3D Delaunay triangulation. They reconstruct a mesh through a probabilistic space carving approach. The approache works online for small objects, but scales poorly, since the triangulation and the reconstruction are executed for each keyframe.

In general, tetrahedron-based algorithms are volumetric methods  much more compact than voxel-based ones, but the accuracy of the reconstruction still needs a refinement to reach state-of-the-art results, indeed the output surface mesh is used as an initialization to mesh-based methods in \cite{vu_et_al_2012,hiep2009towards,salman2010surface}.
\todo{FIGURE}.
\subsubsection{Level set}
Level set methods define a model of the scene as the zero crossing of a function $f:\mathbf{R}^3\leftarrow\mathbf{R}$ \cite{faugeras2002variational,jin2002variational,yezzi2003stereoscopic,fuhrmann2014floating,solem2005geometric,yoon2010joint,pons2007multi}. 
Usually the bootstrap the reconstruction from an initial surface and they evolve it by a variational approach which aims at minimize an energy function.

For instance, in \cite{faugeras2002variational} the evolution of the surface, defined as zero crossing of $f$, is described by partial differential equation of the function $f$. A point on the surface moves proportionally to a value depending on the photo-consistency of its neighborhood: in particular, the more the neighbors are photo-consistent, the ore the point is near to zero.
The work in \cite{jin2002variational} extends this approach by managing surfaces with specular reflection thanks to a convenient choice of the pairwise cameras involved in the photo-consistency computation. In \cite{yoon2010joint} both the reflectiveness and the shape of the scene eas estimated in the variational framework.

In both approaches the photometric measure is integrated in the 3D domain. In \cite{yezzi2003stereoscopic} the authors shows how the integration of this measure on the image domain produce more accurate results.

Solem \etal \cite{solem2005geometric} propose a geometric formalization of the variational minimization, in particular replacing the partial differential equation with the gradient, showing its robustness.
In \cite{pons2005modelling} non rigid scene were addressed, improved in \cite{pons2007multi}  with a new variational framework that takes into account a global matching score, instead of the local presented by the previous approaches.

A recent work of Fuhrmann \etal proposes a new floating scale implicit function which integrates in the reconstruction  points together with their scale, \eg, patch size, which may differ from one point to another. 


Level set methods lead to accurate results, handles implicitly topology changes, but often relies on a memory consumption volumetric initialization, and the evolution of the surface is not always easy to track and understand.
\subsubsection{Mesh}

The last class of MVS algorithms represents the scene as a surface mesh. Some of the methods of the previous paragraphs outputs a mesh, but is always estimated from a different representation which play the major role in the reconstruction process.

Instead the following methods deals directly with a mesh recovered from point clouds, depth maps or as a evolution of an initial mesh surface.

The early Mesh Zippering algorithm \cite{turk1994zippered} creates a triangular mesh from depth maps. It registers all the point clouds with a variant of the ICP (Iterative Closest Point); it creates a mesh for each depth map and finally merge all the meshes.

Mesh from depth images are directly estimated in \cite{pollefeys_et_al_08} too, by applying a coarse-to-fine approach for each depth camera: they bootstraps from a 3D rectangle mesh made up by two triangles that covers all the depth image, each triangle is subdivided and its position re-estimated whenever discontinuity or non-planarity in the depth map region enclosed in the triangle occurs. The meshes generated for each depth maps are registered thanks to the sensor calibration, and the redundant triangles are deleted.

Finally, Bradley \etal \cite{bradley2008accurate} propose to triangulate clusters of the point cloud estimated via depth maps, estimate a mesh out of the triangulation and merge the sub maps created.

Generally, these algorithms estimates a mesh for each depth maps and merge the surfaces mesh to obtain a final model.
A more direct approach that avoids the mesh merging procedure, which may lead to artifacts, other mesh-based algorithms rely on the volumetric approaches presented in \ref{sec:tet-based}.

A deeply different class of mesh-based algorithms aims at refine a coarse mesh of the scene, for instance the visual hull \cite{hiep2009towards,zaharescu2007transformesh,delaunoy_et_al_08,gargallo2007minimizing,delaunoy2011gradient,vu2011large}. These algorithms are similar to the level-set method but directly evolve the mesh in 3D, without the need to extract the surface from an implicit function representation.

The methods proposed in \cite{delaunoy_et_al_08,gargallo2007minimizing,delaunoy2011gradient} evolve the surface to minimize the energy \eqref{eq:eqVoxel} rewriting the photo-consistency term for an image $I$ of camera $i$ as:
\begin{equation}
\label{eq:generative}
  E_{\text{data}}(S) = \int_{\mathit{I}} \rho\left(I(\mathit{u}), I^*_{\Omega}(\mathit{u})\right) d\mathit{u}
\end{equation},
where $\rho$ is a photo-consistency measure, $I$ is the image while $I^*$ is the current estimated surface and texture, rendered in camera $i$.
This equation integrates the photo-consistency in the image domain while most of other level set methods integrate this measure in the surface domain. This lead to more natural Bayesian formalization of the reconstruction. 
However, these approaches, named generative, have the drawback of not implicitly considering the occlusions which makes the functional difficult to be optimized. They need an explicit management of the visibility as in \cite{delaunoy2011gradient}.

A more straightforward approach to surface evolution was proposed by \cite{hiep2009towards} and \cite{vu2011large} which minimize an energy between pairwise images $i$ and $j$:
\begin{equation}
\label{eq:nongen}
  E_{\text{data}}(S) = \int_{\Omega^{\mathit{S}}_{ij}} \rho\left(I(\mathit{u}), I^{\mathit{S}}_{ij}(\mathit{u})\right) d\mathit{u}
\end{equation}
where the image $I^{\mathit{S}}_{ij}$ is the backprojection of the image of camera $j$ to camera $i$ through the current estimated surface $\mathit{S}$, \ie, $I^{\mathit{S}}_{ij} = I_j \circ \Pi_j \circ \Pi_i^{-1}$ , $\Omega^{\mathit{S}}_{ij}$ represent the domain of the surface where the reprojection is defined.

Both the approaches optimize the energies \eqref{eq:generative} and \eqref{eq:nongen} via gradient descent as described in Section \ref{subsec:variational}:

These mesh-based methods have proved to lead to very accurate results and  by estimating  sub-maps and then merge them together as in \cite{vu2011large},  large-scale reconstruction is feasible both on computational and memory sides.
\todo{FIGURE}.

\subsection{Initialization}
As stated in the review of the previous section, many  multi-view algorithms, especially the variational-based ones, bootstrap the optimization from an initial estimate of the 3D model; the initialization is a key point to reach accurate reconstructions.

A common and simple method to initialize MVS algorithm relies on the visual hull \cite{laurentini1994visual}. 
The visual hull is computed by removing the background pixels of each image and by intersecting the cones from the camera centers through the silhouettes. 
The output is an approximate model of the scene, which however is computed very fast, and sufficiently close to the optimized solution: these reasons lead the visual hull to become very popular in MVS literature \cite{jin2002variational,soatto2003tales,zaharescu2007transformesh,yoon2010joint}.

The visual hull requires the silhouettes of the captured object: they exist only if the images capture a single object, or a ver limited amount of objects.
In a generic scene, the silhouettes are very hard to compute or they neither exist.
A more general approach to initialize MVS algorithm builds a surface upon a point cloud estimated for instance via plane sweeping, Structure from Motion or by depth map estimation.
For instance Furukawa \etal \cite{fu10} estimate very reliable 3D points and expand their estimate in the neighborhood, finally the build a Poisson Reconstruction. 
Labatut \etal \cite{labatut2007efficient} and Vu \etal \cite{vu_et_al_2012} build a Delaunay triangulation upon a very dense point cloud, by minimizing an energy functional via graph cuts.

Especially in surface evolution setting, a very desirable property for a robust initialization is manifoldness, in order to avoid degeneracies in the optimization procedure. 
This issue has not be faced neither by \cite{fu10} and by \cite{labatut2007efficient,vu_et_al_2012}: both Poisson Reconstruction and graph cut may lead to non-manifoldness.

% \subsection{Shape priors}
% \subsection{Large Scale}


\section{Incremental reconstruction} 
The Multi-View Stereo algorithms described in the previous section are designed to be applied in a batch setting, \ie, taking into account the whole set of images together in the complete optimization process.

In some applications, especially in robotics, an incremental reconstruction algorithm is more suitable. For instance, when a robot may need a map built while navigating through an environment, for traversability analysis, localization or understanding the scene semantic.
\subsection{Dense Simultaneous Localization and Mapping}
In Section \ref{sec:slam} we presented two approaches to reconstruct a sparse point cloud of the environment together with the camera trajectory and how the two problems become closer and closer: the optimization tool adopted in the batch algorithms of the classical structure from motion, became more and more common in the incremental SLAM algorithms \cite{mouragnon_et_al07,strasdat11}.

Similarly, since the proposals of Newcombe \etal \cite{newcombe2010live,newcombe2011kinectfusion,newcombe2011dtam} and St{\"u}ckler \etal \cite{stuhmer2010real}, Multi-View Stereo becomes closer to the robotics community: variational optimization on depth maps estimation and fusion has been performed in parallel with a camera tracking thread.

Newcombe \etal \cite{newcombe2010live} proposed to use structure from motion points and camera poses extracted incrementally by PTAM \cite{klein_murray07}, and to build an initial surface by interpolating the points with radial basis function as \cite{ohtake2003multi}; then by GPU dense optical flow, they estimate the difference between the predicted image with the initial surface and the observation (the images), to update the model.

KinectFusion \cite{newcombe2011kinectfusion} deals with RGB-D data, \ie, range data acquired  directly and coded in the images. It registers the points from the range data to the estimated surface, and updates  a volumetric Truncated Signed Distance Function (TSDF) according the data. DTAM \cite{newcombe2011dtam} extended the same approach to RGB by proposing a robust approach to estimate range data in real time exploiting the power of GPU processing.

Recently \cite{bylow2013real} improved the registration of RGB-D data of the KinectFusion approach by exploiting the information encoded in the TDSF, and \cite{concha2015incorporating} improved on DTAM by incorporating planar and layout priors in the estimation. The author in \cite{stuhmer2012parallel,stuckler2014multi} proposed different approaches to estimate and register depth maps in real time.

Most of the previous approaches rely on data from RGB-D cameras, not always available, for instance in outdoor environment; moreover they adopt depth map  volumetric representation that results to be not scalable for large-scale scenes.  

\subsection{Incremental reconstruction from sparse points}
A different class of incremental algorithms builds a dense model of the environment directly on the SfM points.
Incremental 3D reconstruction from a sparse point cloud is gaining interest in the computer vision community as incremental Structure from Motion algorithms are consolidating  \cite{wu13}, especially where GPU-demanding Dense SLAM methods cannot be applied. 
% This is clearly true for those applications where a rough, but dense, surface represents a sufficient and effective representation of the scene, e.g, for traversability analysis in unmanned vehicle navigation. 
% Furthermore, in real-time applications, the map of the environment needs to be updated online, and the surface has to be estimated incrementally. 

Most of the existing incremental algorithms \cite{Lovi_et_al_11,Pan_et_al09,litvinov_lhuillier_13,litvinov_Lhiuller14} bootstrap the reconstruction of a mesh surface from the 3D Delaunay triangulation of a sparse point cloud. Indeed, the Delaunay property, \ie, no point of the triangulation is inside the sphere circumscribing any tetrahedron, avoids as much as possible the resulting tetrahedra to have a degenerate shape \cite{Maur_02}; it is self-adaptive, \ie, the more the points are dense the more the tetrahedra are small; it is very fast to compute, and to  update against point removal or addition; off-the-shelf libraries, such as CGAL \cite{cgal}, enable a very simple and efficient management of it. 

As soon as a Delaunay triangulation is available, several approaches exist to extract a surface taking into account the visibility of each point. 
We recall the simple space carving \cite{kutulakos_seitz05}: it initializes all the tetrahedra as \emph{matter}, then it marks as \emph{free space} the tetrahedra intersected by the camera-to-point \emph{viewing rays}, \ie, the lines from the camera center to the observed 3D points in the triangulation. 
The boundary between free space and matter represents the final surface of the scene.
Pan et al. \cite{Pan_et_al09} improve upon this simple procedure by proposing an online probabilistic Space Carving, but this is not an incremental approach: they start from scratch every time new points are added.
Lovi et al. \cite{Lovi_et_al_11} present the first incremental Space Carving algorithm which runs real-time, but, as for the previous methods, the estimated surface is not guaranteed to be manifold.

Several reasons lead to enforce the manifold property \cite{lhuillier20152}. 
Most Computer Graphics algorithms need the manifold property, for instance smoothing with Laplace-Beltrami operator \cite{Meyer03}, or the linear mesh parametrization \cite{saboret00}.
Moreover the manifold property enables surface evolution in mesh-based Multi-View Stereo, as in \cite{vu_et_al_2012,delaunoy_et_al_08}.the manifold property enables a photometric refinement by surface evolution such as with the high accurate Multi-View Stereo mesh-based algorithm as in \cite{vu_et_al_2012,delaunoy_et_al_08}.
With these approaches is hard to estimate the surface evolving flow in the presence of non manifold vertices: indeed they compute for each vertex the gradient minimizing the reprojection error, by summing-up the contribution of the incident facets; if the vertex is not manifold, this gradient does not converge. As a further proof of this, \cite{vu_et_al_2012} needs to manually fix the surface estimated via s-t cut.
As in \cite{vu_et_al_2012}, it is possible to fix the mesh as a post-processing step, but reconstructing directly a manifold as in the proposed paper, enables the design of a fully automatic pipeline which do not need human intervention.

In literature, the only algorithm reconstructing a manifold incrementally was proposed by Litvinov and Lhuiller \cite{litvinov_lhuillier_13,litvinov_Lhiuller14} and extended in \cite{romanoni15a,romanoni15b}. 
In their work, the authors  bootstrap from the Space Carving procedure and, by taking into account the number of intersections of each tetrahedron with the viewing rays, they reconstruct a surface keeping the manifold property valid. 
The main limitation is that  Litvinov and Lhuiller insert a point into the Delaunay triangulation only when its position is definitive, then they cannot move the point position anymore even in the case they could refine their estimate. 
The main reason of Litvinov and Lhuiller design choice has to be ascribed to the computational cost of updating the visibility information along the viewing rays incident to each moved point, and the computational cost of updating part of the Delaunay triangulation, which in turn induces a new manifold reconstruction iteration step. This limitation has been faced in \cite{romanoni15a}.

% Indeed, the very common approach to deal with a point moving in the triangulation, is to remove it and add it back in the new position \cite{cgal} (Fig. \ref{fig:moving}). 
% When we remove a point (the point A in Fig. \ref{fig:moving}(a)) and we want to keep the Delaunay property, we have to remove all the tetrahedra incident to that point (light red triangles in Fig. \ref{fig:moving}(b)); then, we add a new set of tetrahedra to triangulate the resulting hole (dark green triangles in \ref{fig:moving}(c)).
% When we add a new point into the triangulation  (the point B in Fig. \ref{fig:moving}(d)), a set of tetrahedra would conflict with it, \ie, the Delaunay property is broken (light red triangles in Fig. \ref{fig:moving}(d)); so, we remove this set of tetrahedra again (red triangles in Fig. \ref{fig:moving}(e)) and we add a new connected set that re-triangulate the hole (dark green triangles in Fig. \ref{fig:moving}(f)).
% Whenever a set of tetrahedra is replaced, we have to transfer conveniently the information about the visibility (matter or free space) of the removed tetrahedra to the new one. 
% In addition to this, we have to update the visibility of the tetrahedra crossed by a visibility ray from one camera to the moved point.
% For these reasons the update of the point position is computational demanding.

To complete the overview of the incremental reconstruction methods from sparse data, we mention here another very different approach proposed by Hoppe et al. \cite{Hoppe13} who label the tetrahedra with a random field, and extract the surface via graph-cuts by minimizing a visibility-consistent energy function. This incremental algorithm is effective and handles the moving points, but the manifold property of the reconstructed surface is not yet guaranteed.

% \section{other sensors}



