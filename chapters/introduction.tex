
\chapter{Introduction}
\label{sec:intro}
We are in the era of autonomous driving: vehicles endowed with a wide variety of sensors, are becoming  capable to navigate in more and more challenging environments.
As the humans use mostly vision among their senses to perceive and act, the vehicles could heavily rely on images perceived by the cameras to interact with the surrounding.
Indeed, many advancements towards the autonomous driving have been achieved thanks to the joint efforts of Computer Vision and Robotics communities.
Computer Vision aims at extracting meaningful information from images such as the class the subject belongs to, the action happening in the scene or the 3D model of the environment, while Robotics aims at designing robots, or autonomous machines, and at deploying them in the real world.

One of the key aspect in autonomous driving is the knowledge of the map of the environment. 
In Robotics and Computer Vision, visual mapping and 3D reconstruction, represent two very active research fields, which both aims at modeling the scene captured in the images.
Early works in Robotics focus especially on the real-time creation of the map while the camera is navigating through an environment, leading to the so called Simultaneous Localization and Mapping (SLAM) algorithms. 
On the other hand, Computer Vision researchers tries to recover the  camera poses together with the structure of the map, with the Structure from Motion (SfM) algorithms, by processing all the images at the same time.
However, since the proposal of the Parallel Tracking and Mapping (PTAM) algorithm in \cite{klein_murray07} the methods adopted by the two communities became closer and closer.

SfM and SLAM algorithms build a point-wise map of the environment that is often very sparse, except in the case of semi-dense SLAM, such as LSD-SLAM \cite{engel2014lsd}. 
Only few Dense-SLAM proposals, as DTAM \cite{newcombe2011dtam} or \cite{newcombe2010live}, are able to recover a dense representation of the scene, however they scales badly in large-scale environments, therefor they are not suitable for autonomous driving.

In Computer Vision, the so called Multi-View Stereo (MVS) algorithms are able to recover a large-scale mesh of the environment from the images. 
MVS decouples the camera pose estimation and the model computation: it delegates the former task to an external algorithm, such as a SfM or a sensor fusion algorithm as \cite{mouragnon_et_al07} or \cite{cucci_matteucci13}, and it focuses on the estimation of an accurate and detailed model of the environment.

As in the early SfM,  Multi-View Stereo limits its application to off-line, batch processing. No MVS algorithm runs incrementally, since the focus is addressed only to accuracy.
However, incremental dense mapping would be very useful in various applications. 
For instance during mapping campaign, the operators need to inspect the map while the images are  acquired in order to understand if more data have to be collected, e.g., were details are missing. 
Incremental mapping is also needed whenever an existing map has to me updated with images capturing new region of the environment, or with new images that add details about an existing part of the map.

\section{Motivation}
In the last decade, many Multi-View Stereo approaches have been proposed; they differ from various aspects, e.g., initialization, optimization procedure, 3D model representation.

The scene can be represented by different geometric entities: point, 3D patches, volumes or surfaces.
Points are adopted when the computational time is the limited resource, e.g., in SLAM, and algorithm relying on 3D patches show impressive results on highly textured scenes. 
These representations cannot be adopted for robot navigation, since they are sparse, therefore not a continuous navigable surface.
%All the feature-based and semi-dense SLAM approaches, for computational reasons output such a sparse result, therefor their map are not suitable 

Volumetric and mesh-based reconstruction are the most common in the Multi-View Stereo community; they especially get a boost from the widespread availability of GPU hardware that enable massive parallel processing.
The former algorithms partition the scene into voxels or tetrahedra and define which subset is the free space and which is the matter, the latter bootstrap from an initial mesh, which is evolved in order to minimize pairwise photometric error, \ie, the error between one image and the projection of a second image through the mesh to the first image.

Volumetric algorithms achieve very accurate results but their application is limited to small scene, since their data structure is not scalable. 
The best trade off between scalability and accuracy is achieved by the mesh-based algorithms which have been proven to score state of the art accuracy \cite{li2015detail} and to reconstruct large-scale scenes \cite{vu_et_al_2012}.


Mesh-based algorithms requires an initial mesh, which would be evolved by the optimization algorithm; a widespread initialization is known as visual hull, the reconstruction is the intersection of cones with the vertex in the camera and tangent to the perceived object.
Visual hull needs the knowledge of the silhouette of the objects, which is not a realistic assumption for real world scenarios.
Different approaches have been proposed to initialize a general scene after initialization but the process requires human intervention to fix non manifold vertices, because they induce inconsistencies in the mesh refinement.

Even if the Multi-View stereo algorithms described thus far achieve very accurate reconstructions, they need to process all the images at the same time. 
In order to fill the gap between this field and the Robotics SLAM, we want to propose an incremental algorithm which builds the map while the robot, e.g., a surveying vehicle, is navigating through the environment. 
This approach aims at providing an updated detailed map as soon as possible.



\section{Thesis contributions}
In this thesis we overcome the issue described thus far and we started to build a bridge between Robotics, and mesh-based Multi-View Stereo.
We propose the first automatic and incremental pipeline able to build a dense, scalable and manifold mesh.
The mesh representation is necessary to reconstruct  large-scale scenes and, by keeping the manifold property valid along the whole processing, we are also able to design a completely automatic pipeline.

The building blocks of our pipeline are essentially two: incremental reconstruction from Sparse Points and incremental refinement.
The former estimates a  manifold mesh from camera poses, sparse point clouds and camera-to-point viewing rays, e.g., the output of a Structure from Motion algorithm.
The algorithm relies on a volumetric representation, in particular the Delaunay triangulation; each tetrahedron is voted consistently with the rays ant the manifold mesh is extracted as the boundary between those which receive high votes (free space) and those traversed by non or few rays (matter).
We investigated which kind of 3D points are suitable for Delaunay Triangulation for 3D reconstruction; since 3D points on the real-world edges lead the edges of Delaunay tetrahedra to lay on them, we proved that, when the output is a video, 2D edge-points on the images are the most convenient feature to be reconstructed in 3D.
We also proposed a new voting scheme which improves the accuracy of the reconstruction with respect to the state of the art, and we now efficiently handle moving point inside the triangulation.

In the incremental refinement we propose a novel approach to build a dense and detailed map of the environment. 
We present this contribution as a first attempt to fill the gap between the dense SLAM and Multi-View Stereo.
While all of the Multi-View Stereo algorithm process all the images  at the same time, and Robotics dense SLAM algorithms works in small environment, we reconstruct small and large scenes incrementally. 
In this module of the pipeline we evolve photometrically the part of the scene yet estimated by the previous step, once the refinement is over, we merge it with the new mesh from the sparse point cloud with a novel merging algorithm which is able to keep the manifold property valid.

Aside from this incremental pipeline this thesis presents to other relevant contributions.
First, we noticed that the manifold mesh estimated from the sparse data can sometimes get the refinement to be stucked  in local minima or in other cases can cause a slow convergence. 
We investigated how to overcome this issue and improve the accuracy to move the initialization closer to the real solution.
We found that by adding few accurate 3D points in the sparse point clouds can significantly speed up the convergence and achieve better results.
For this purpose, we sweep the initial mesh in the space and we extract a 3D point where the matching score induced in a pair of camera is very high, which likely means that this point belongs to the real scene.

Second, since lidar data are often available together with images in autonomous driving applications, we tested and extended our mapping pipeline to work with hybrid lidar and image data.
We propose a complete mapping framework  that leverages on the high accuracy of the 3D lidar data and on the appearance and density of the images.
By detecting removing moving points from lidar point clouds, we reconstruct the map of the scene with our manifold reconstruction algorithm; we refine it by neglecting the moving objects from the photometric optimization. Finally we recover a full textured map where moving points where filtered out.



\section{Thesis outline}

The thesis is organized in three parts. 
In the first part we provide all the background materials needed to completely understand the contributions of the thesis.
In Chapter \ref{ch:computational} we give an overview of the computational geometry tools we use in particular to describe ho we reconstruct a manifold mesh.  
Chapter \ref{ch:camera} contains the classic definitions of pin-hole camera and the geometry behind the  multi-view setting, we also present how to formulate the pin-hole model in OpenGL.
In Chapter \ref{ch:soa} we review the state of the art in mapping and Multi-View Stereo, focusing on the dichotomy between the Robotics and  Computer Vision algorithms propose to solve the same mapping problem from two different perspectives

The second part of the thesis presents the incremental pipeline. 
In Chapter \ref{ch:incrDenseRef} we show the incremental manifold reconstruction algorithm which, from a sparse point cloud produces an initialization for the incremental refinement described in  Chapter \ref{ch:computational}.

In the third part of the thesis we illustrate our contribution aside the incremental algorithm, which are closely related to the mesh-based reconstruction.
In particular Chapter \ref{ch:sweeping} shows how to improve the speed and the accuracy and convergence speed through a novel mesh sweeping algorithm.
In Chapter \ref{ch:laser} we present our approach extended to work with lidar data: we explain how we remove the moving point from the lidar point cloud and how we leverage on this information to estimate a refined and textured mesh.
In Chapter \ref{ch:further} we finally show some further experiments in which our algorithm plays a major role, in particular we show how the dense map can be adopted to localize a camera.














