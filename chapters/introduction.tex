
\chapter*{Introduction}
\label{sec:intro}
We are in the era of autonomous driving. Vehicles are becoming endowed with more and more sensors, whose signals are processed to navigate in the surrounding.
As humans use mostly vision among their senses to perceive and act, the cars, the robots or in general the machines could heavily rely on image data to interact with the environment.
Computer Vision aims at extracting meaningful information from images such as the class the subject belongs to, the action happening in the scene or the 3D model of the environment, while Robotics aims at designing robots, or autonomous machines, and at deploying them in the real world.
Thanks to the joint efforts of these two communities many advancements have been achieved in different aspect enabling the autonomous driving.

One of the key aspect needed to navigate through an environment is the knowledge of the map of the environment itself. 
While many Simultaneous Localization and Mapping algorithms have been proposed to navigate through an environment without the map, in real world scenario we need to decouple these aspects in order to achieve both robustness and speed.
In principle, if a map of the environment is known, the robot is able to localize itself and navigate through them following a path defined by the user.

In Robotics and Computer Vision, visual mapping, or 3D reconstruction, is a long standing field of research: it aims at building a 3D model of environment assuming that the pose of the camera is known from external calibration, for instance from explicit measurements, from telemetry or self-calibration algorithms.


multi view

robotics mostly slam


dicotomia robotics e computer vision
%Robotics and Computer Vision pursue different aims 
% Mapping from images is an ill-posed problem since from 2D data the aim is to recover a 3D structure.

\section{Motivation}
In the last decade, many mapping approaches have been proposed; they differ from various aspects, e.g., initialization, optimization procedure, 3D model representation.

The scene can be represented by different geometric entities: point, 3D patches, volumes or surfaces.
Points are adopted when the computational time is the limited resource, for instance, in Robotics when Simultaneous Localization and Mapping, and algorithm relying on 3D patches show impressive results on highly textured scenes. 
These representations cannot be adopted for robot navigation, since they are sparse, therefore not a continuous navigable surface.
%All the feature-based and semi-dense SLAM approaches, for computational reasons output such a sparse result, therefor their map are not suitable 

Volumetric and mesh-based reconstruction are the most common in the Multi-View Stereo community;they especially get a boost from the widespread availability of GPU hardware that enable massive parallel processing.
The former algorithms partition the scene into voxels or tetrahedra and define which subset is the free space and which is the matter, the latter bootstrap from an initial mesh, which is evolved in order to minimize pairwise photometric error, i.e., the error between one image and the projection of a second image through the mesh to the first image.

Volumetric algorithms achieve very accurate results but their application is limited to small scene, since their data structure is not scalable. 
The best trade off between scalability and accuracy is achieved by the mesh-based algorithms which have been proven to score state of the art accuracy \cite{li2015detail} and to reconstruct large-scale scenes \cite{vu_et_al_2012}.


Mesh-based algorithms requires an initial mesh, which would be evolved by the optimization algorithm; a widespread initialization is known as visual hull, the reconstruction is the intersection of cones with the vertex in the camera and tangent to the perceived object.
Visual hull needs the knowledge of the silhouette of the objects, which is not a realistic assumption for real world scenarios.
Different approaches have been proposed to initialize a general scene after initialization but the process requires human intervention to fix non manifold vertices, because they induce inconsistencies in the mesh refinement.

Even if the Multi-View stereo algorithms described thus far achieve very accurate reconstructions, they need to process all the images at the same time. 
In order to fill the gap between this field and the Robotics SLAM, we want to propose an incremental algorithm which builds the map while the robot, e.g., a surveying vehicle, is navigating through the environment. 
This approach aims at providing an updated detailed map as soon as possible.
A typical scenario could be a mapping campaign.
As we want to inspect the map while we are acquiring the data, e.g., to collect more data were details are missing, we need to visualize the reconstruction on-the-field. 


\section{Thesis contributions}
In this thesis we overcome the issue described thus far and we started to build a bridge between Robotics, and mesh-based Multi-View Stereo.
We propose the first automatic and incremental pipeline able to build a dense, scalable and manifold mesh.
The mesh representation is necessary to reconstruct  large-scale scenes and, by keeping the manifold property valid along the whole processing, we are also able to design a completely automatic pipeline.

The building blocks of our pipeline are essentially two: incremental reconstruction from Sparse Points and incremental refinement.
The former estimates a  manifold mesh from camera poses, sparse point clouds and camera-to-point viewing rays, e.g., the output of a Structure from Motion algorithm.
The algorithm relies on a volumetric representation, in particular the Delaunay triangulation; each tetrahedron is voted consistently with the rays ant the manifold mesh is extracted as the boundary between those which receive high votes (free space) and those traversed by non or few rays (matter).
We investigated which kind of 3D points are suitable for Delaunay Triangulation for 3D reconstruction; since 3D points on the real-world edges lead the edges of Delaunay tetrahedra to lay on them, we proved that, when the output is a video, 2D edge-points on the images are the most convenient feature to be reconstructed in 3D.
We also proposed a new voting scheme which improves the accuracy of the reconstruction with respect to the state of the art, and we now efficiently handle moving point inside the triangulation.

In the incremental refinement we propose a novel approach to build a dense and detailed map of the environment. 
We present this contribution as a first attempt to fill the gap between the dense SLAM and Multi-View Stereo.
While all of the Multi-View Stereo algorithm process all the images  at the same time, and Robotics dense SLAM algorithms works in small environment, we reconstruct small and large scenes incrementally. 
In this module of the pipeline we evolve photometrically the part of the scene yet estimated by the previous step, once the refinement is over, we merge it with the new mesh from the sparse point cloud with a novel merging algorithm which is able to keep the manifold property valid.

Aside from this incremental pipeline this thesis presents to other relevant contributions.
First, we noticed that the manifold mesh estimated from the sparse data can sometimes get the refinement to be stucked  in local minima or in other cases can cause a slow convergence. 
We investigated how to overcome this issue and improve the accuracy to move the initialization closer to the real solution.
We found that by adding few accurate 3D points in the sparse point clouds can significantly speed up the convergence and achieve better results.
For this purpose, we sweep the initial mesh in the space and we extract a 3D point where the matching score induced in a pair of camera is very high, which likely means that this point belongs to the real scene.

Second, since lidar data are often available together with images in autonomous driving applications, we tested and extended our mapping pipeline to work with hybrid lidar and image data.
We propose a complete mapping framework  that leverages on the high accuracy of the 3D lidar data and on the appearance and density of the images.
By detecting removing moving points from lidar point clouds, we reconstruct the map of the scene with our manifold reconstruction algorithm; we refine it by neglecting the moving objects from the photometric optimization. Finally we recover a full textured map where moving points where filtered out.



\section{Thesis outline}

The thesis is organized in three parts. 
In the first part we provide all the background materials needed to completely understand the contributions of the thesis.
In Chapter \ref{ch:computational} we give an overview of the computational geometry tools we use in particular to describe ho we reconstruct a manifold mesh.  
Chapter \ref{ch:camera} contains the classic definitions of pin-hole camera and the geometry behind the  multi-view setting, we also present how to formulate the pin-hole model in OpenGL.
In Chapter \ref{ch:soa} we review the state of the art in mapping and Multi-View Stereo, focusing on the dichotomy between the Robotics and  Computer Vision algorithms propose to solve the same mapping problem from two different perspectives

The second part of the thesis presents the incremental pipeline. 
In Chapter \ref{ch:incrDenseRef} we show the incremental manifold reconstruction algorithm which, from a sparse point cloud produces an initialization for the incremental refinement described in  Chapter \ref{ch:computational}.

In the third part of the thesis we illustrate our contribution aside the incremental algorithm, which are closely related to the mesh-based reconstruction.
In particular Chapter \ref{ch:sweeping} shows how to improve the speed and the accuracy and convergence speed through a novel mesh sweeping algorithm.
In Chapter \ref{ch:laser} we present our approach extended to work with lidar data: we explain how we remove the moving point from the lidar point cloud and how we leverage on this information to estimate a refined and textured mesh.
In Chapter \ref{ch:further} we finally show some further experiments in which our algorithm plays a major role, in particular we show how the dense map can be adopted to localize a camera.














